library(readr)     # read files
library(dplyr)     # data manip
library(tibble)    # work with dataframe
library(tidyr)     # data manip
library(ggplot2)   # plotting
library(stringr)   # work with strings
library(tidytext)  # work with text - main functionality
library(textstem)  # stem words
library(tokenizers) # count words
library(reshape2)  # cast from long to wide and vice versa
library(wordcloud) # plot wordclouds
disney_f <- read_csv('../data/DisneylandReviews.csv')
disney_f <- read_csv('data/DisneylandReviews.csv')
View(disney_f)
library(reshape2)   # cast from long to wide and vice versa
# --- Install Libraries --- #
install.packages(c('tidyverse', 'tidytext', 'textstem', 'tokenizers',
'reshape2', 'wordcloud'))
print('hello')
disney_df <- read.csv('../data/DisneylandReviews.csv')
disney_df <- read.csv('data/DisneylandReviews.csv')
library(tidyverse)  # Handle data
library(tidytext)   # work with text - main functionality
library(textstem)   # stem words
library(tokenizers) # count words
library(reshape2)   # cast from long to wide and vice versa
library(wordcloud)  # plot wordclouds
disney_df <- read.csv('data/DisneylandReviews.csv')
View(disney_df)
disney_df <- disney_df %>%
mutate(n_words = count_words(Review_Text), # create column with word count
n_char = nchar(Review_Text))        # create column with character count
disney_df %>%
group_by(Branch)
disney_df %>%
group_by(Branch) %>%
count(n)
disney_df %>%
group_by(Branch) %>%
count(n())
# BASIC PLOT SETUP - We'll fill this in
# length of reviews by a quick cut of individual rating
disney_df %>%
ggplot() +
geom_histogram(
aes(x = n_words)
) +
theme_bw() +
scale_x_continuous(trans = 'log1p') # log1p if there are a lot of zeros in the data
# BASIC PLOT SETUP - We'll fill this in
# length of reviews by a quick cut of individual rating
disney_df %>%
ggplot() +
geom_histogram(
aes(x = n_words)
) +
facet_wrap(~Branch)
theme_bw() +
scale_x_continuous() # log1p if there are a lot of zeros in the data
theme_bw() # log1p if there are a lot of zeros in the data
theme_bw() +
scale_x_continuous(trans = 'log1p')
# BASIC PLOT SETUP - We'll fill this in
# length of reviews by a quick cut of individual rating
disney_df %>%
ggplot() +
geom_histogram(
aes(x = n_words)
) +
facet_wrap(~Branch)
tokens <- disney_df %>%
unnest_tokens(word, Review_Text) %>%
select(Review_ID, Rating)
View(tokens)
tokens <- disney_df %>%
unnest_tokens(word, Review_Text)
View(tokens)
common_words <- tokens %>% # Create dataset t
group_by(word) %>%
count(sort = TRUE)
View(common_words)
disney_df <- disney_df %>%
mutate(sentiment = case_when(
Rating > 3 ~ 'Positive',
Rating == 3 ~ 'Neutral',
Rating < 3 ~ 'Negative'
))
# --- Counting Characters and Approx. Word Counts --- #
# Count Words and characters
disney_df <- disney_df %>%
mutate(n_words = count_words(Review_Text), # create column with word count
n_char = nchar(Review_Text))        # create column with character count
# BASIC PLOT SETUP - We'll fill this in
# length of reviews by a quick cut of individual rating
disney_df %>%
ggplot() +
geom_histogram(
aes(x = n_words)
) +
facet_wrap(~Branch)
tokens <- disney_df %>%
unnest_tokens(word, Review_Text)
common_words <- tokens %>% # Create dataset t
group_by(word) %>%
count(sort = TRUE)
View(tokens)
disney_df <- group_by(Branch, sentiment) %>%
count(n())
disney_df <- group_by(Branch, sentiment) %>%
count(n())
disney_df <- group_by(Branch) %>%
count(n())
disney_df <- group_by(disney_df$Branch) %>%
count(n())
disney_df %>%
group_by(Branch, sentiment) %>%
count(n())
count(
disney_df %>%
group_by(Branch, sentiment) %>%
count()
disney_df %>%
disney_df %>%
group_by(Branch, sentiment) %>%
n()
disney_df %>%
group_by(Branch, sentiment) %>%
n()
disney_df %>%
group_by(Branch, sentiment) %>%
count(n())
tokens_no_stop <- tokens %>%
anti_join(stop_words) %>%
filter(!str_detect(word, "[[:digit:]]+"))
View(tokens_no_stop)
common_words_2 <- tokens_no_stop %>%
group_by(word) %>%
count(sort = TRUE)
View(common_words_2)
#--- Custom Stop Words --- #
custom_stop_words <-  # words that have no real value, all reviews are about doctors
tibble(
word = c(
'park',
'parks',
'disney',
'disneyland',
'paris',
'california',
'florida',
'world',
'land',
'visit',
'visits',
'visited',
'trip',
'day',
'days',
'time',
'times',
'people',
'lot',
'lots',
'bit',
'main',
'start',
'book',
'absolutely',
'taking',
'managed',
'decided',
'makes',
'real',
'life',
'ago',
# also remove sentiment fillers
'amazing',
'love',
'loved',
'magival',
'magic',
'nice',
'enjoyed',
'enjoy',
'worth',
'recommend',
'beautiful',
'happiest',
'excellent',
'favorit',
'lovely',
'perfect',
'highly'
),
lexicon = 'docs' # Create lexicon !!!!
)
tokens_no_stop <- tokens_no_stop %>%
anti_join(custom_stop_words)
# Plot Common Words after stopword removal
tokens_no_stop %>%
group_by(word) %>%
count(sort = TRUE) %>%
ungroup() %>%
top_n(25) %>%
ggplot(aes(x = n,
y = reorder(word, n)
)
) +
geom_col() +
scale_y_reordered() +
labs(y = NULL)
tokens_stemmed <- tokens_no_stop %>%
mutate(word_stem = stem_words(word),
word_lemma = lemmatize_words(word))
View(tokens_stemmed)
# --- Word Clouds --- #
tokens_stemmed %>%
filter(sentiment_star != 'neutral') %>%
count(word_lemma, sentiment, sort = TRUE) %>%
acast(word_lemma ~ sentiment, value.var = "n", fill = 0) %>%
commonality.cloud(colors = c("red", "blue"),
max.words = 50)
# --- Word Clouds --- #
tokens_stemmed %>%
filter(sentiment != 'Neutral') %>%
count(word_lemma, sentiment, sort = TRUE) %>%
acast(word_lemma ~ sentiment, value.var = "n", fill = 0) %>%
commonality.cloud(colors = c("red", "blue"),
max.words = 50)
# --- Word Clouds --- #
tokens_stemmed %>%
filter(sentiment != 'Neutral') %>%
count(word_lemma, sentiment, sort = TRUE) %>%
acast(word_lemma ~ sentiment, value.var = "n", fill = 0) %>%
commonality.cloud(#colors = c("red", "blue"),
max.words = 50)
# --- Word Clouds --- #
tokens_stemmed %>%
filter(sentiment != 'Neutral') %>%
count(word_lemma, sentiment, sort = TRUE) %>%
acast(word_lemma ~ sentiment, value.var = "n", fill = 0) %>%
commonality.cloud(),
# --- Word Clouds --- #
tokens_stemmed %>%
filter(sentiment != 'Neutral') %>%
count(word_lemma, sentiment, sort = TRUE) %>%
acast(word_lemma ~ sentiment, value.var = "n", fill = 0) %>%
commonality.cloud(max.words = 75)
# --- Save Data --- #
write_csv(tokens_stemmed, '../data/tokenized_reviews.csv')
# --- Save Data --- #
write_csv(tokens_stemmed, 'data/tokenized_reviews.csv')
# --- Load Data --- #
tokens <- read_csv('data/tokenized_reviews.csv')
reviews <- read_csv('data/DisneylandReviews.csv')
# --- Vader Sentiment --- #
vader_sent <- vader_df(reviews$Review_Text)
# --- Library --- #
library(tidyverse)  # Handle data
library(tidytext)   # work with text
library(vader)      # VADER sentiment lexicon -> better at social media text
library(tokenizers) # tokenizers
library(textdata)   # "Standard" Sentiment Lexicons
library(textstem)   # word stemmer
library(yardstick)  # assess performance
# --- Vader Sentiment --- #
vader_sent <- vader_df(reviews$Review_Text)
word_counts <- tokens %>%
group_by(word_lemma) %>%
count(sort = TRUE) %>%
filter(n >= 4) # reduce noise / model will be faster
View(word_counts)
word_counts <- tokens %>%
group_by(word_lemma) %>%
count(sort = TRUE) %>%
filter(n >= 6) # reduce noise / model will be faster
word_counts <- tokens %>%
group_by(word_lemma) %>%
count(sort = TRUE) %>%
filter(n >= 8) # reduce noise / model will be faster
word_counts <- tokens %>%
group_by(word_lemma) %>%
count(sort = TRUE) %>%
filter(n >= 10) # reduce noise / model will be faster
tokens <- tokens %>%
filter(word_lemma %in% word_counts$word_lemma)
reviews_processed <- tokens %>%
group_by(Review_ID)
View(reviews_processed)
reviews_word_count <- tokens %>%
count(id, word_lemma) %>%
ungroup()
reviews_word_count <- tokens %>%
count(Review_ID, word_lemma) %>%
ungroup()
View(reviews_word_count)
reviews_dtm <- reviews_word_count %>% # exam
cast_sparse(Review_ID, word_lemma, n)
View(reviews_dtm)
reviews_processed <- tokens %>%
group_by(Review_ID)
reviews_processed <- tokens %>%
group_by(Review_ID) %>%
summarise(
review_clean = str_c(word_lemma, collapse = ' '),
.groups = 'drop'
)
View(reviews_processed)
reviews_processed <- disney_df %>%
left_join(reviews_processed, by ='Review_ID')
View(reviews_processed)
# --- Save Data --- #
write_csv(tokens_stemmed, 'data/DisneyReviews_processed.csv')
# --- Save Data --- #
write_csv(reviews_processed, 'data/DisneyReviews_processed.csv')
View(reviews_processed)
disney_df %>%
group_by(Branch) %>%
n()
disney_df %>%
group_by(Branch) %>%
count()
reviews_processed <- tokens %>%
group_by(Review_ID) %>%
summarise(
review_clean = str_c(word_lemma, collapse = ' '),
.groups = 'drop'
)
print('Script was succesfull!')
# --- Install Libraries --- #
install.packages(c('tidyverse', 'tidytext', 'textstem', 'tokenizers',
'reshape2', 'wordcloud'))
